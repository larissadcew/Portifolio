[![.github/workflows/ci.yaml](https://github.com/pages-themes/minimal/actions/workflows/ci.yaml/badge.svg)](https://github.com/pages-themes/minimal/actions/workflows/ci.yaml) [![Gem Version](https://badge.fury.io/rb/jekyll-theme-minimal.svg)](https://badge.fury.io/rb/jekyll-theme-minimal)

*Minimal is a Jekyll theme for GitHub Pages. You can [preview the theme to see what it looks like](http://pages-themes.github.io/minimal), or even [use it today](#usage).*

![Thumbnail of Minimal](thumbnail.png)

<img src="Fotolinkedin.png" alt="Thumbnail of Minimal" width="200" style="float: left; margin-right: 20px;" />

## ğŸ‘‹ğŸ¼ Hello there, Iâ€™m Larissa!


ğŸ‘¨ğŸ»â€ğŸ’» Iâ€™m a final year graduate student at the TU Berlin

ğŸ”¬ My research interests are in bridging vision and language modalities and Self-Supervised Learning!

ğŸ“½ï¸ I am also interested in assisting others on their path in the world of Machine Learning and academia.
## Selected Experience

ğŸ¤– Open Source Contributions
I have experience contributing to Arena Bench a large open-source project for robotic obstacle avoidance using Deep Reinforcement Learning.

Moreover, I have published a respective paper at the IROS conference and in the Robotics and Automation Letters (RA-L) journal.

### ğŸ“œ Reimplementing and Reproducing Papers

I have experience with independent research. I have implemented the Reward Constrained Policy Optimization paper into stable-baselines3 PPO and reproduced the original results by running and tracking experiments.

To accompany this work, I have submitted a blog post to the ICLR Blogposts Track communicating the paperâ€™s theory and my results.
